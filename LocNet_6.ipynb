{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LocNet 6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iGQiD-05U2P"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras import Model, Sequential\n",
        "from keras.layers import Dense, Input, Conv2D, Dropout, Layer, ReLU, MaxPool2D, Flatten\n",
        "from keras.metrics import CategoricalAccuracy, CategoricalCrossentropy\n",
        "from itertools import product\n",
        "import shutil"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8JC4Htc6Eoi"
      },
      "source": [
        "dtype = tf.float32\n",
        "intDtype = tf.int32\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "trainDataGen = ImageDataGenerator(rescale=1.0/255.0, dtype= dtype, validation_split= 2.00075/10) # For some reason 0.2 does not works for 20% split.\n",
        "batchSize = 100\n",
        "inputSize = 49\n",
        "gridSize = 49"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoLZhKQr6Gw3"
      },
      "source": [
        "dirName = '/content/affineMNIST'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8c966wH6Ln8"
      },
      "source": [
        "shutil.unpack_archive('/content/affineMNIST.zip',extract_dir= dirName)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ufc0Rju6MCt",
        "outputId": "3ca052a1-1b73-48c5-c305-28d2a3ebb142"
      },
      "source": [
        "trainGenerator = trainDataGen.flow_from_directory(dirName, target_size= (inputSize,inputSize), color_mode= 'grayscale', batch_size = batchSize, shuffle = True, seed = 42, subset='training')\n",
        "testGenerator = trainDataGen.flow_from_directory(dirName, target_size= (inputSize, inputSize), color_mode = 'grayscale', batch_size = batchSize, shuffle = True, seed = 42, subset = 'validation')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 48000 images belonging to 10 classes.\n",
            "Found 12000 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5qfFElX6RhP"
      },
      "source": [
        "LocNet = Sequential([\n",
        "   Input(shape=(inputSize,inputSize,1)),\n",
        "   Conv2D(7, (3, 3), strides=(2,2), padding=\"same\", activation= 'relu', bias_initializer = 'glorot_uniform'),\n",
        "   Conv2D(7, (5, 5), strides=(2,2), padding='same', activation= 'relu', bias_initializer = 'glorot_uniform'),\n",
        "   MaxPool2D(),\n",
        "   Flatten(),\n",
        "   Dense(30, activation= 'relu'),\n",
        "   Dense(6)\n",
        "], name = 'LocalisationNetwork')\n",
        "LocNet.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtvIeU5L6WEj"
      },
      "source": [
        "CNN = Sequential([\n",
        "    Input(shape=(gridSize,gridSize,1)),\n",
        "    Conv2D(7, (7, 7), strides=(2,2), padding=\"same\", activation= 'relu',bias_initializer = 'glorot_uniform'),\n",
        "    MaxPool2D(),\n",
        "    Flatten(),\n",
        "    Dense(10,activation='softmax')\n",
        "], name = 'CNN')\n",
        "CNN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z3yiclN6gAt"
      },
      "source": [
        "networkLoss = keras.losses.categorical_crossentropy"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8HKEvDl6iAG"
      },
      "source": [
        "adamLearningRate = 0.0005\n",
        "\n",
        "cnnOptimiser = keras.optimizers.Adam(learning_rate=adamLearningRate)\n",
        "stnOptimiser = keras.optimizers.Adam(learning_rate=adamLearningRate)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zHiAW8_6jgf"
      },
      "source": [
        "def sampler(input, batchTheta):\n",
        "  delta = 1e-9\n",
        "  P1, P2, P3, P4, P5, P6 = tf.unstack(batchTheta, axis = 1)\n",
        "  zeros, ones = tf.zeros(batchSize, dtype = dtype), tf.ones(batchSize, dtype = dtype)\n",
        "\n",
        "  thetas = tf.stack([[P1 + ones, P2, P3], [P4, P5 + ones, P6], [zeros, zeros, ones]])\n",
        "  thetas = tf.transpose(thetas, perm = [2, 0, 1])\n",
        "  # (None, 3,3)\n",
        "  indexRange = (gridSize - 1)/2\n",
        "  vGrid = tf.meshgrid(tf.linspace(-indexRange, indexRange, gridSize), tf.linspace(-indexRange, indexRange, gridSize))\n",
        "  \n",
        "  vGrid = tf.cast(vGrid, dtype = dtype)\n",
        "  xvGrid, yvGrid = tf.reshape(vGrid[0], (1, -1)), tf.reshape(vGrid[1], (1, -1))\n",
        "  vGrid = tf.stack([xvGrid, yvGrid, tf.ones_like(yvGrid, dtype = dtype)], axis = 1)\n",
        "  vGrid = tf.tile(vGrid, [batchSize, 1, 1])\n",
        "  # (b*3*gridSize**2) = (b*3*3) * (b*3*gridSize**2)\n",
        "  Gs = tf.matmul(thetas, vGrid)\n",
        "  Xs, Ys, scale = tf.unstack(Gs, axis= 1)\n",
        "  Xs, Ys = tf.reshape(Xs/(scale + delta), [batchSize, gridSize, gridSize]), tf.reshape(Ys/(scale + delta), [batchSize, gridSize, gridSize])\n",
        "\n",
        "  XFloor, YFloor = tf.floor(Xs), tf.floor(Ys)\n",
        "  XCeil, YCeil = tf.math.ceil(Xs), tf.math.ceil(Ys)\n",
        "  XfloorInt, YfloorInt = tf.cast(XFloor, intDtype), tf.cast(YFloor, intDtype)\n",
        "  XceilInt, YceilInt = tf.cast(XCeil, intDtype), tf.cast(YCeil, intDtype)\n",
        "  imageIndex = np.tile(np.arange(batchSize).reshape([batchSize,1,1]),[1,inputSize,inputSize])\n",
        "  imageCopy = tf.reshape(input,[-1,int(input.shape[-1])])\n",
        "  imagePadded = tf.concat([imageCopy,tf.zeros([1,int(input.shape[-1])])],axis=0)\n",
        "\n",
        "  firstQuarterIndex = (imageIndex*inputSize+YfloorInt)*inputSize+XfloorInt\n",
        "  secondQuarterIndex = (imageIndex*inputSize+YfloorInt)*inputSize+XceilInt\n",
        "  thirdQuarterIndex = (imageIndex*inputSize+YceilInt)*inputSize+XfloorInt\n",
        "  fourthQuarterIndex = (imageIndex*inputSize+YceilInt)*inputSize+XceilInt\n",
        "  paddingIndex = tf.fill([batchSize,inputSize,inputSize],batchSize*inputSize**2)\n",
        "\n",
        "  xFloorRange = tf.logical_and(XfloorInt >= 0, XfloorInt < inputSize)\n",
        "  yFloorRange = tf.logical_and(YfloorInt >= 0, YfloorInt < inputSize)\n",
        "  xCeilRange = tf.logical_and(XceilInt >= 0, XceilInt < inputSize)\n",
        "  yCeilRange = tf.logical_and(YceilInt >= 0, YceilInt < inputSize)\n",
        "\n",
        "  firstQuarter = tf.logical_and(xFloorRange, yFloorRange)\n",
        "  secondQuarter = tf.logical_and(xFloorRange, yCeilRange)\n",
        "  thirdQuarter = tf.logical_and(xCeilRange, yFloorRange)\n",
        "  fourthQuarter = tf.logical_and(xCeilRange, yCeilRange)\n",
        "   \n",
        "  firstQuarterIndex = tf.where(firstQuarter,firstQuarterIndex,paddingIndex)\n",
        "  secondQuarterIndex = tf.where(secondQuarter,secondQuarterIndex,paddingIndex)\n",
        "  thirdQuarterIndex = tf.where(thirdQuarter,thirdQuarterIndex,paddingIndex)\n",
        "  fourthQuarterIndex = tf.where(fourthQuarter,fourthQuarterIndex,paddingIndex)\n",
        "\n",
        "  Xratio = tf.reshape(Xs-XFloor,[batchSize,inputSize,inputSize,1])\n",
        "  Yratio = tf.reshape(Ys-YFloor,[batchSize,inputSize,inputSize,1])\n",
        "  \n",
        "  firstImage = tf.cast(tf.gather(imagePadded,firstQuarterIndex), dtype)*(1-Xratio)*(1-Yratio)\n",
        "  secondImage = tf.cast(tf.gather(imagePadded,secondQuarterIndex), dtype)*(Xratio)*(1-Yratio)\n",
        "  thirdImage = tf.cast(tf.gather(imagePadded,thirdQuarterIndex), dtype)*(1-Xratio)*(Yratio)\n",
        "  fourthImage = tf.cast(tf.gather(imagePadded,fourthQuarterIndex), dtype)*(Xratio)*(Yratio)\n",
        "  imageUnWarped = firstImage+secondImage+thirdImage+fourthImage\n",
        "\n",
        "  return imageUnWarped\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg3-cUoqBjQC"
      },
      "source": [
        "def trainStep(U, label):\n",
        "  varU = tf.Variable(U, trainable= True)\n",
        "\n",
        "  with tf.GradientTape(persistent=True) as samplingTape:\n",
        "    params = LocNet(varU)\n",
        "    V = sampler(varU, params)\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as classificationTape:\n",
        "      predictedLabel = CNN(V)\n",
        "      loss = networkLoss(label, predictedLabel)\n",
        "      loss = tf.reduce_mean(loss)\n",
        "\n",
        "  cnnGradients = classificationTape.gradient(loss, CNN.trainable_weights)\n",
        "  stnGradients = samplingTape.gradient(loss, LocNet.trainable_weights)\n",
        "\n",
        "  cnnOptimiser.apply_gradients(zip(cnnGradients, CNN.trainable_weights))\n",
        "  stnOptimiser.apply_gradients(zip(stnGradients, LocNet.trainable_weights))\n",
        "\n",
        "  del samplingTape, classificationTape\n",
        "\n",
        "  return loss, predictedLabel, V"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm8B2xNyBn8V"
      },
      "source": [
        "def testStep(U, label):\n",
        "  varU = tf.Variable(U, trainable= False)\n",
        "  params = LocNet(varU)\n",
        "  V = sampler(varU, params)\n",
        "\n",
        "  predictedLabel = CNN(V)\n",
        "  valLoss = networkLoss(label, predictedLabel)\n",
        "  valLoss = tf.reduce_mean(valLoss)\n",
        "\n",
        "  return valLoss, predictedLabel"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBFYDOQiBruR"
      },
      "source": [
        "epochAccuracy = CategoricalAccuracy()\n",
        "epochLoss = CategoricalCrossentropy()\n",
        "epochCount = 1\n",
        "highAccuracy = False\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  while epochCount > 0:\n",
        "    print(f'Epoch - {epochCount}')\n",
        "    for step in range(len(trainGenerator)):\n",
        "      genU, genLabel = trainGenerator[step]\n",
        "      loss, predictedLabel, V = trainStep(genU, genLabel)\n",
        "\n",
        "      predictedIndices = tf.argmax(predictedLabel, axis = 1)\n",
        "      actualIndices = tf.argmax(genLabel, axis = 1)\n",
        "      epochAccuracy.update_state(actualIndices, predictedIndices)\n",
        "      epochLoss.update_state(actualIndices, predictedIndices)\n",
        "\n",
        "      if step % 100 == 1:\n",
        "        print(f'Step : {step}, loss : {loss}')\n",
        "\n",
        "    acc = epochAccuracy.result().numpy()    \n",
        "    print(f'\\nEpoch Accuracy : {acc*100} %')\n",
        "    epoLoss = epochLoss.result().numpy()\n",
        "    print(f'Epoch Loss : {epoLoss/(len(trainGenerator)*batchSize)}\\n')\n",
        "    epochCount += 1\n",
        "    epochAccuracy.reset_states()\n",
        "    epochLoss.reset_states()\n",
        "\n",
        "    if acc > 0.90:\n",
        "      if highAccuracy:\n",
        "        break\n",
        "      else:\n",
        "        highAccuracy = True\n",
        "    else:\n",
        "      highAccuracy = False "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RhfPQQPBwcJ"
      },
      "source": [
        "epochTestAccuracy = CategoricalAccuracy()\n",
        "epochTestLoss = CategoricalCrossentropy()\n",
        "with tf.device('/gpu:0'):\n",
        "  for batch in range(len(testGenerator)):\n",
        "    genU, genLabel = testGenerator[batch]\n",
        "    valLoss, predictedLabel = testStep(genU, genLabel)\n",
        "    predictedIndices = tf.argmax(predictedLabel, axis = 1)\n",
        "    actualIndices = tf.argmax(genLabel, axis = 1)\n",
        "\n",
        "    epochTestAccuracy.update_state(actualIndices, predictedIndices)\n",
        "    epochTestLoss.update_state(actualIndices, predictedIndices)\n",
        "\n",
        "    #print(f'For batch no. {batch + 1}, val loss. {valLoss}')\n",
        "\n",
        "  print(f'Test Accuracy {epochTestAccuracy.result().numpy()*100}')\n",
        "  print(f'Test Loss {epochTestLoss.result().numpy()/(len(testGenerator)*batchSize)}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}